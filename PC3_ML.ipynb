{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vD_5sjyg508C",
        "outputId": "17b68e77-7731-4d3f-b6c3-ef766f78929c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import json"
      ],
      "metadata": {
        "id": "FtWmQbYy-qHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "Se está utilizando el dataset \"Recyclable and Household Waste Classification Dataset\" obtenido de Kaggle.\n",
        "\n",
        "Link: https://www.kaggle.com/datasets/alistairking/recyclable-and-household-waste-classification\n"
      ],
      "metadata": {
        "id": "VVJMNAgp_w-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"alistairking/recyclable-and-household-waste-classification\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iq1WrS7m7BVp",
        "outputId": "b19a5757-993b-4f40-b414-6d195977aca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'recyclable-and-household-waste-classification' dataset.\n",
            "Path to dataset files: /kaggle/input/recyclable-and-household-waste-classification\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_PATH = \"/kaggle/input/recyclable-and-household-waste-classification/images/images\"\n"
      ],
      "metadata": {
        "id": "iGFosSYSAZHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparación del dataset\n",
        "El dataset original contiene 15000 imágenes organizadas uniformemente en 30 clases. Se decidió reorganizar las carpetas de forma que queán en las siguientes 6 clases:\n",
        "- biodegradable\n",
        "- glass\n",
        "- metal\n",
        "- non_recyclable\n",
        "- paper\n",
        "- plastic"
      ],
      "metadata": {
        "id": "dOoJFseVAWc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Diccionario de clases\n",
        "\n",
        "CLASS_MAP = {\n",
        "    \"biodegradable\": [\n",
        "        \"coffee_grounds\",\n",
        "        \"eggshells\",\n",
        "        \"food_waste\",\n",
        "        \"tea_bags\"\n",
        "    ],\n",
        "    \"plastic\": [\n",
        "        \"disposable_plastic_cutlery\",\n",
        "        \"plastic_cup_lids\",\n",
        "        \"plastic_detergent_bottles\",\n",
        "        \"plastic_food_containers\",\n",
        "        \"plastic_shopping_bags\",\n",
        "        \"plastic_soda_bottles\",\n",
        "        \"plastic_straws\",\n",
        "        \"plastic_trash_bags\",\n",
        "        \"plastic_water_bottles\"\n",
        "    ],\n",
        "    \"paper\": [\n",
        "        \"cardboard_boxes\",\n",
        "        \"cardboard_packaging\",\n",
        "        \"magazines\",\n",
        "        \"newspaper\",\n",
        "        \"office_paper\",\n",
        "        \"paper_cups\"\n",
        "    ],\n",
        "    \"metal\": [\n",
        "        \"aerosol_cans\",\n",
        "        \"aluminum_food_cans\",\n",
        "        \"aluminum_soda_cans\",\n",
        "        \"steel_food_cans\"\n",
        "    ],\n",
        "    \"glass\": [\n",
        "        \"glass_beverage_bottles\",\n",
        "        \"glass_cosmetic_containers\",\n",
        "        \"glass_food_jars\"\n",
        "    ],\n",
        "    \"non_recyclable\": [\n",
        "        \"clothing\",\n",
        "        \"shoes\",\n",
        "        \"styrofoam_cups\",\n",
        "        \"styrofoam_food_containers\"\n",
        "    ]\n",
        "}\n"
      ],
      "metadata": {
        "id": "phUAtUdTGQx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Organización de carpetas"
      ],
      "metadata": {
        "id": "pRR25ZnRG5gk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_PATH = \"dataset_clean\"\n",
        "# Crear estructura de salida para las carpetas train, val y test\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "    for cls in CLASS_MAP.keys():\n",
        "        os.makedirs(os.path.join(OUTPUT_PATH, split, cls), exist_ok=True)\n",
        "\n",
        "print(\"Generando dataset limpio...\\n\")\n",
        "\n",
        "# Procesar cada categoría de los diccionarios\n",
        "for final_class, original_classes in CLASS_MAP.items():\n",
        "    print(f\"Procesando categoría → {final_class}\")\n",
        "\n",
        "    all_images = []\n",
        "\n",
        "    # Reunir imagenes de cada clase original\n",
        "    for orig in original_classes:\n",
        "        orig_path = os.path.join(BASE_PATH, orig)\n",
        "        if not os.path.exists(orig_path):\n",
        "            print(f\"No existe: {orig_path}\")\n",
        "            continue\n",
        "\n",
        "        # leer default/ y real_world/\n",
        "        for sub in [\"default\", \"real_world\"]:\n",
        "            subfolder = os.path.join(orig_path, sub)\n",
        "            if not os.path.exists(subfolder):\n",
        "                continue\n",
        "\n",
        "            files = [\n",
        "                os.path.join(subfolder, f)\n",
        "                for f in os.listdir(subfolder)\n",
        "                if f.endswith(\".png\")\n",
        "            ]\n",
        "\n",
        "            all_images.extend(files)\n",
        "\n",
        "    print(f\"  → Total imágenes encontradas: {len(all_images)}\")\n",
        "\n",
        "    # Dividir en train/val/test\n",
        "    train_files, temp = train_test_split(all_images, test_size=0.3, random_state=42)\n",
        "    val_files, test_files = train_test_split(temp, test_size=0.5, random_state=42)\n",
        "\n",
        "    # Copiar archivos\n",
        "    for f in train_files:\n",
        "        shutil.copy(f, os.path.join(OUTPUT_PATH, \"train\", final_class))\n",
        "\n",
        "    for f in val_files:\n",
        "        shutil.copy(f, os.path.join(OUTPUT_PATH, \"val\", final_class))\n",
        "\n",
        "    for f in test_files:\n",
        "        shutil.copy(f, os.path.join(OUTPUT_PATH, \"test\", final_class))\n",
        "\n",
        "print(\"\\n✔ Dataset preparado en:\", OUTPUT_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Gc_DzJ4FFF4",
        "outputId": "3a6f082c-4810-4878-b47c-9fb37b3b2534"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generando dataset limpio...\n",
            "\n",
            "Procesando categoría → biodegradable\n",
            "  → Total imágenes encontradas: 2000\n",
            "Procesando categoría → plastic\n",
            "  → Total imágenes encontradas: 4500\n",
            "Procesando categoría → paper\n",
            "  → Total imágenes encontradas: 3000\n",
            "Procesando categoría → metal\n",
            "  → Total imágenes encontradas: 2000\n",
            "Procesando categoría → glass\n",
            "  → Total imágenes encontradas: 1500\n",
            "Procesando categoría → non_recyclable\n",
            "  → Total imágenes encontradas: 2000\n",
            "\n",
            "✔ Dataset preparado en: dataset_clean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Total de imágenes en la carpeta test"
      ],
      "metadata": {
        "id": "KRCNAwY_HqJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_PATH = \"dataset_clean/test\"\n",
        "\n",
        "# Obtener todas las clases\n",
        "classes = sorted(os.listdir(DATASET_PATH))\n",
        "print(\"Clases detectadas:\", classes)\n",
        "\n",
        "# Contar imágenes por clase\n",
        "class_counts = {}\n",
        "for cls in classes:\n",
        "    path = os.path.join(DATASET_PATH, cls)\n",
        "    count = len(os.listdir(path))\n",
        "    class_counts[cls] = count\n",
        "    print(f\"{cls}: {count} imágenes\")\n",
        "\n",
        "# Crear etiquetas repetidas según frecuencia\n",
        "labels = []\n",
        "for idx, cls in enumerate(classes):\n",
        "    labels.extend([idx] * class_counts[cls])\n",
        "labels = np.array(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViBq6TQWHL9G",
        "outputId": "8080c53c-9fcb-4ff6-8c91-65036ee3024e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clases detectadas: ['biodegradable', 'glass', 'metal', 'non_recyclable', 'paper', 'plastic']\n",
            "biodegradable: 177 imágenes\n",
            "glass: 153 imágenes\n",
            "metal: 177 imágenes\n",
            "non_recyclable: 177 imágenes\n",
            "paper: 219 imágenes\n",
            "plastic: 236 imágenes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pesos por clase"
      ],
      "metadata": {
        "id": "IVJZXpJuJSZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear etiquetas según frecuencia\n",
        "labels = []\n",
        "for idx, cls in enumerate(classes):\n",
        "    labels.extend([idx] * class_counts[cls])\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Calcular class weights\n",
        "weights = compute_class_weight(class_weight=\"balanced\",\n",
        "                               classes=np.arange(len(classes)),\n",
        "                               y=labels)\n",
        "class_weights = {i: w for i, w in enumerate(weights)}\n",
        "\n",
        "print(\"\\nClass weights calculados:\")\n",
        "for cls_idx, weight in class_weights.items():\n",
        "    print(f\"{classes[cls_idx]}: {weight:.2f}\")\n",
        "\n",
        "# Guardar class weights para entrenamiento\n",
        "with open(\"class_weights.json\", \"w\") as f:\n",
        "    json.dump(class_weights, f)\n",
        "print(\"\\n✔ Class weights guardados en 'class_weights.json'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGQ0vvWGHmGr",
        "outputId": "3f34223b-b7ce-459e-f255-df83358fd05b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Class weights calculados:\n",
            "biodegradable: 1.00\n",
            "glass: 1.00\n",
            "metal: 1.00\n",
            "non_recyclable: 1.00\n",
            "paper: 1.00\n",
            "plastic: 1.00\n",
            "\n",
            "✔ Class weights guardados en 'class_weights.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El dataset está balanceado y cada clase contiene un número similar de muestras. Por ello, no es necesario hacer cambios adicionales para compensar desbalances."
      ],
      "metadata": {
        "id": "t9bIVrQuJfwV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento"
      ],
      "metadata": {
        "id": "C7j7SBLsITXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "9amLR7JiIRcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rutas\n",
        "DATASET_PATH = \"dataset_clean\"\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 20"
      ],
      "metadata": {
        "id": "wAs3H0pVIiaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "\n",
        "# augmentation para train\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# SOLO preprocess para val/test\n",
        "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n"
      ],
      "metadata": {
        "id": "lBWqbGqqMUN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generadores\n",
        "train_ds = train_datagen.flow_from_directory(\n",
        "    DATASET_PATH + \"/train\",\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=\"categorical\"\n",
        ")\n",
        "\n",
        "val_ds = val_datagen.flow_from_directory(\n",
        "    DATASET_PATH + \"/val\",\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=\"categorical\"\n",
        ")\n",
        "\n",
        "test_ds = test_datagen.flow_from_directory(\n",
        "    DATASET_PATH + \"/test\",\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=False\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4t4saErJVt-",
        "outputId": "ac63103f-cc41-4e94-f6a8-f83db6aba618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1500 images belonging to 6 classes.\n",
            "Found 1165 images belonging to 6 classes.\n",
            "Found 1139 images belonging to 6 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modelo base\n",
        "base_model = EfficientNetB0(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False,\n",
        "    input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
        ")\n",
        "base_model.trainable = False"
      ],
      "metadata": {
        "id": "LtrIQt6OJyxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cabeza del modelo\n",
        "x = GlobalAveragePooling2D()(base_model.output)\n",
        "x = Dense(256, activation=\"relu\")(x)\n",
        "output = Dense(6, activation=\"softmax\")(x)\n",
        "model = Model(inputs=base_model.input, outputs=output)"
      ],
      "metadata": {
        "id": "JuOil3wPPJ3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento"
      ],
      "metadata": {
        "id": "Bkhg7Hc-2hgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n",
        "\n",
        "\n",
        "# compilar (fase 1)\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Entrenamiento\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[checkpoint]\n",
        ")\n",
        "\n",
        "# Guardar modelo\n",
        "model.save(\"/content/drive/MyDrive/PC3/PC3/modelo_2/efficientnet_waste.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a5RQLQ9J8yo",
        "outputId": "845c4cca-c684-4a58-e943-2c6c9637dd33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648ms/step - accuracy: 0.4901 - loss: 1.3916"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 1s/step - accuracy: 0.4932 - loss: 1.3855 - val_accuracy: 0.7708 - val_loss: 0.6981\n",
            "Epoch 2/20\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 433ms/step - accuracy: 0.8136 - loss: 0.6106"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 507ms/step - accuracy: 0.8138 - loss: 0.6098 - val_accuracy: 0.8240 - val_loss: 0.5223\n",
            "Epoch 3/20\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 441ms/step - accuracy: 0.8764 - loss: 0.4136"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 514ms/step - accuracy: 0.8762 - loss: 0.4136 - val_accuracy: 0.8455 - val_loss: 0.4552\n",
            "Epoch 4/20\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 433ms/step - accuracy: 0.9001 - loss: 0.3462"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 505ms/step - accuracy: 0.9001 - loss: 0.3459 - val_accuracy: 0.8584 - val_loss: 0.4115\n",
            "Epoch 5/20\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 503ms/step - accuracy: 0.9164 - loss: 0.2904 - val_accuracy: 0.8532 - val_loss: 0.4052\n",
            "Epoch 6/20\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 439ms/step - accuracy: 0.9359 - loss: 0.2461"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 512ms/step - accuracy: 0.9359 - loss: 0.2460 - val_accuracy: 0.8592 - val_loss: 0.3854\n",
            "Epoch 7/20\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 497ms/step - accuracy: 0.9594 - loss: 0.2013 - val_accuracy: 0.8584 - val_loss: 0.3949\n",
            "Epoch 8/20\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 433ms/step - accuracy: 0.9609 - loss: 0.1892"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 507ms/step - accuracy: 0.9607 - loss: 0.1892 - val_accuracy: 0.8670 - val_loss: 0.3772\n",
            "Epoch 9/20\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 499ms/step - accuracy: 0.9675 - loss: 0.1437 - val_accuracy: 0.8644 - val_loss: 0.3764\n",
            "Epoch 10/20\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 439ms/step - accuracy: 0.9727 - loss: 0.1368"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 512ms/step - accuracy: 0.9726 - loss: 0.1368 - val_accuracy: 0.8687 - val_loss: 0.3710\n",
            "Epoch 11/20\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 505ms/step - accuracy: 0.9771 - loss: 0.1211 - val_accuracy: 0.8635 - val_loss: 0.3771\n",
            "Epoch 12/20\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 437ms/step - accuracy: 0.9803 - loss: 0.1112"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 509ms/step - accuracy: 0.9802 - loss: 0.1112 - val_accuracy: 0.8747 - val_loss: 0.3723\n",
            "Epoch 13/20\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 507ms/step - accuracy: 0.9804 - loss: 0.0925 - val_accuracy: 0.8730 - val_loss: 0.3690\n",
            "Epoch 14/20\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 501ms/step - accuracy: 0.9842 - loss: 0.0817 - val_accuracy: 0.8738 - val_loss: 0.3611\n",
            "Epoch 15/20\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 501ms/step - accuracy: 0.9836 - loss: 0.0813 - val_accuracy: 0.8678 - val_loss: 0.3708\n",
            "Epoch 16/20\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 496ms/step - accuracy: 0.9912 - loss: 0.0745 - val_accuracy: 0.8721 - val_loss: 0.3732\n",
            "Epoch 17/20\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 504ms/step - accuracy: 0.9868 - loss: 0.0673 - val_accuracy: 0.8747 - val_loss: 0.3691\n",
            "Epoch 18/20\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 440ms/step - accuracy: 0.9909 - loss: 0.0633"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 514ms/step - accuracy: 0.9908 - loss: 0.0634 - val_accuracy: 0.8781 - val_loss: 0.3838\n",
            "Epoch 19/20\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 438ms/step - accuracy: 0.9924 - loss: 0.0588"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 512ms/step - accuracy: 0.9923 - loss: 0.0588 - val_accuracy: 0.8798 - val_loss: 0.3775\n",
            "Epoch 20/20\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 510ms/step - accuracy: 0.9845 - loss: 0.0592 - val_accuracy: 0.8738 - val_loss: 0.3738\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Métricas"
      ],
      "metadata": {
        "id": "T0lIorchq-xd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluación en test\n",
        "loss, accuracy = model.evaluate(test_ds)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "print(\"Test Loss:\", loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOL4CBfVrAoo",
        "outputId": "3f019194-ca09-42e5-b139-9640210d5649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - accuracy: 0.8927 - loss: 0.3646\n",
            "Test Accuracy: 0.8665496110916138\n",
            "Test Loss: 0.3966206908226013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "\n",
        "true_labels = test_ds.classes\n",
        "class_names = list(test_ds.class_indices.keys())\n",
        "\n",
        "# Predicciones\n",
        "pred_probs = model.predict(test_ds)\n",
        "pred_labels = np.argmax(pred_probs, axis=1)\n",
        "\n",
        "# matriz de confusion\n",
        "cm = confusion_matrix(true_labels, pred_labels)\n",
        "\n",
        "print(\"=== Matriz de confusión ===\")\n",
        "print(cm)\n",
        "\n",
        "# Reporte por clase\n",
        "print(\"\\n=== Accuracy por clase ===\")\n",
        "report = classification_report(true_labels, pred_labels, target_names=class_names)\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGZGsP594YPb",
        "outputId": "f1a87693-f9e2-4c1d-dcb5-ca3f4968f882"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 76ms/step\n",
            "=== Matriz de confusión ===\n",
            "[[162   1   1   1   7   5]\n",
            " [  1 134   5   0   2  11]\n",
            " [  4   8 159   1   1   4]\n",
            " [  2   0   2 154  11   8]\n",
            " [  6   0   5  14 189   5]\n",
            " [  6  16  11   5   9 189]]\n",
            "\n",
            "=== Accuracy por clase ===\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            " biodegradable       0.90      0.92      0.91       177\n",
            "         glass       0.84      0.88      0.86       153\n",
            "         metal       0.87      0.90      0.88       177\n",
            "non_recyclable       0.88      0.87      0.88       177\n",
            "         paper       0.86      0.86      0.86       219\n",
            "       plastic       0.85      0.80      0.83       236\n",
            "\n",
            "      accuracy                           0.87      1139\n",
            "     macro avg       0.87      0.87      0.87      1139\n",
            "  weighted avg       0.87      0.87      0.87      1139\n",
            "\n"
          ]
        }
      ]
    }
  ]
}